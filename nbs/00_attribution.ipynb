{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135c844b",
   "metadata": {},
   "source": [
    "# Attribution--the attribution toolkit\n",
    "> Notebook for attribution testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db10403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import kaggle\n",
    "import timm\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from captum.attr import IntegratedGradients, Occlusion, FeaturePermutation\n",
    "from scipy.ndimage import gaussian_filter\n",
    "# from captum.attr._utils.masking import_mask # Corrected import\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f64c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle only if it does not exist\n",
    "dataset_dir = './data/barhill'\n",
    "if not os.path.exists(dataset_dir):\n",
    "    kaggle.api.dataset_download_files('mshahoyi/barhills-processed', path='./data', unzip=True)\n",
    "    print(\"Dataset downloaded and unzipped.\")\n",
    "else:\n",
    "    print(\"Dataset already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47651508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained MegaDescriptor model\n",
    "model_name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n",
    "model = timm.create_model(model_name, num_classes=0, pretrained=True).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_path = './data/barhill/gallery_and_probes.csv'\n",
    "df = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two random newts\n",
    "unique_newts = df['newt_id'].unique()\n",
    "np.random.seed(42)\n",
    "random_newts = np.random.choice(unique_newts, 2, replace=False)\n",
    "# random_newts[0] and random_newts[1] are guaranteed to be different newt IDs\n",
    "# (assuming there are at least 2 unique newts in your dataset).\n",
    "print(f\"Selected newts: {random_newts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample images for each newt\n",
    "newt1_images = df[df['newt_id'] == random_newts[0]]['image_path'].values[:2]\n",
    "newt2_images = df[df['newt_id'] == random_newts[1]]['image_path'].values[:2]\n",
    "\n",
    "print(f\"Newt 1 images: {newt1_images}\")\n",
    "print(f\"Newt 2 images: {newt2_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653114c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Plot the selected images for visual confirmation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "fig.suptitle(\"Selected Images for Analysis\", fontsize=16)\n",
    "\n",
    "# Load and preprocess images\n",
    "def get_full_image_path(rel_path):\n",
    "    return os.path.join('./data', rel_path)\n",
    "\n",
    "\n",
    "# Newt 1, Image 1\n",
    "img_n1_i1 = Image.open(get_full_image_path(newt1_images[0])).convert('RGB')\n",
    "axes[0, 0].imshow(img_n1_i1)\n",
    "axes[0, 0].set_title(f\"Newt {random_newts[0]} - Image 1\\n{newt1_images[0]}\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Newt 1, Image 2\n",
    "if len(newt1_images) > 1:\n",
    "    img_n1_i2 = Image.open(get_full_image_path(newt1_images[1])).convert('RGB')\n",
    "    axes[0, 1].imshow(img_n1_i2)\n",
    "    axes[0, 1].set_title(f\"Newt {random_newts[0]} - Image 2\\n{newt1_images[1]}\")\n",
    "    axes[0, 1].axis('off')\n",
    "else:\n",
    "    axes[0, 1].axis('off') # Hide subplot if no second image\n",
    "    axes[0, 1].text(0.5, 0.5, 'No second image', ha='center', va='center')\n",
    "\n",
    "\n",
    "# Newt 2, Image 1\n",
    "img_n2_i1 = Image.open(get_full_image_path(newt2_images[0])).convert('RGB')\n",
    "axes[1, 0].imshow(img_n2_i1)\n",
    "axes[1, 0].set_title(f\"Newt {random_newts[1]} - Image 1\\n{newt2_images[0]}\")\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Newt 2, Image 2\n",
    "if len(newt2_images) > 1:\n",
    "    img_n2_i2 = Image.open(get_full_image_path(newt2_images[1])).convert('RGB')\n",
    "    axes[1, 1].imshow(img_n2_i2)\n",
    "    axes[1, 1].set_title(f\"Newt {random_newts[1]} - Image 2\\n{newt2_images[1]}\")\n",
    "    axes[1, 1].axis('off')\n",
    "else:\n",
    "    axes[1, 1].axis('off') # Hide subplot if no second image\n",
    "    axes[1, 1].text(0.5, 0.5, 'No second image', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc0d3c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(img)\n",
    "    return img, input_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb753fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "newt1_img1, newt1_tensor1 = load_and_preprocess_image(get_full_image_path(newt1_images[0]))\n",
    "newt1_img2, newt1_tensor2 = load_and_preprocess_image(get_full_image_path(newt1_images[1]))\n",
    "newt2_img1, newt2_tensor1 = load_and_preprocess_image(get_full_image_path(newt2_images[0]))\n",
    "newt2_img2, newt2_tensor2 = load_and_preprocess_image(get_full_image_path(newt2_images[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008a292",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimilarityModel(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Get features from both images\n",
    "        features1 = self.backbone(x1)\n",
    "        features2 = self.backbone(x2)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = torch.nn.functional.cosine_similarity(features1, features2)\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9afae0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create the similarity model\n",
    "similarity_model = SimilarityModel(model).to(device)\n",
    "similarity_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46deeef",
   "metadata": {},
   "source": [
    "## Occlusion Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def my_occlusion_sensitivity(model, image1, image2, patch_size=16, stride=8, occlusion_value=0, device=None):\n",
    "    \"\"\"\n",
    "    Perform occlusion sensitivity test on the first image to see which regions\n",
    "    affect similarity with the second image.\n",
    "    \n",
    "    Args:\n",
    "        model: The similarity model\n",
    "        image1: First image tensor (to be occluded) - shape [1, C, H, W]\n",
    "        image2: Second image tensor - shape [1, C, H, W]\n",
    "        patch_size: Size of the occlusion patch\n",
    "        stride: Stride for moving the occlusion patch\n",
    "        occlusion_value: Value used for occlusion (default: 0)\n",
    "        \n",
    "    Returns:\n",
    "        Sensitivity map showing which regions, when occluded, affect similarity the most\n",
    "    \"\"\"\n",
    "\n",
    "    import torch\n",
    "\n",
    "    # Move tensors to the right device\n",
    "    if device is not None:\n",
    "        image1 = image1.to(device)\n",
    "        image2 = image2.to(device)\n",
    "    \n",
    "    # Get the original similarity score\n",
    "    with torch.no_grad():\n",
    "        original_similarity = model(image1, image2).item()\n",
    "    \n",
    "    # Get image dimensions\n",
    "    _, c, h, w = image1.shape\n",
    "    \n",
    "    # Initialize sensitivity map\n",
    "    sensitivity_map = torch.zeros((h, w), device='cpu')\n",
    "    \n",
    "    # Compute number of patches\n",
    "    n_h_patches = (h - patch_size) // stride + 1\n",
    "    n_w_patches = (w - patch_size) // stride + 1\n",
    "    \n",
    "    # Create progress counter\n",
    "    total_patches = n_h_patches * n_w_patches\n",
    "    patch_count = 0\n",
    "    \n",
    "    # Slide the occlusion window over the image\n",
    "    for i in range(0, h - patch_size + 1, stride):\n",
    "        for j in range(0, w - patch_size + 1, stride):\n",
    "            # Create a copy of the image\n",
    "            occluded_image = image1.clone()\n",
    "            \n",
    "            # Apply occlusion\n",
    "            occluded_image[0, :, i:i+patch_size, j:j+patch_size] = occlusion_value\n",
    "            \n",
    "            # Compute the similarity with occlusion\n",
    "            with torch.no_grad():\n",
    "                occluded_similarity = model(occluded_image, image2).item()\n",
    "            \n",
    "            # Calculate the difference (sensitivity)\n",
    "            sensitivity = original_similarity - occluded_similarity\n",
    "            \n",
    "            # Update the sensitivity map\n",
    "            sensitivity_map[i:i+patch_size, j:j+patch_size] += sensitivity\n",
    "            \n",
    "            # Update progress counter\n",
    "            patch_count += 1\n",
    "            if patch_count % 10 == 0:\n",
    "                print(f\"Processed {patch_count}/{total_patches} patches\", end='\\r')\n",
    "    \n",
    "    print(f\"\\nCompleted occlusion testing - {total_patches} patches processed.\")\n",
    "    \n",
    "    # Normalize the sensitivity map for visualization\n",
    "    if sensitivity_map.max() > 0:\n",
    "        sensitivity_map = (sensitivity_map - sensitivity_map.min()) / (sensitivity_map.max() - sensitivity_map.min())\n",
    "    \n",
    "    return sensitivity_map.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40febad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def occlusion_sensitivity(model, image1, image2, patch_size=16, stride=8, occlusion_value=0):\n",
    "    \"\"\"\n",
    "    Perform occlusion sensitivity test on the first image to see which regions\n",
    "    affect similarity with the second image using Captum.\n",
    "    \n",
    "    Args:\n",
    "        model: The similarity model\n",
    "        image1: First image tensor (to be occluded) - shape [1, C, H, W]\n",
    "        image2: Second image tensor - shape [1, C, H, W]\n",
    "        patch_size: Size of the occlusion patch\n",
    "        stride: Stride for moving the occlusion patch\n",
    "        occlusion_value: Value used for occlusion (default: 0)\n",
    "        \n",
    "    Returns:\n",
    "        Sensitivity map showing which regions, when occluded, affect similarity the most\n",
    "    \"\"\"    \n",
    "    # Move tensors to the right device\n",
    "    image1 = image1.to(device)\n",
    "    image2 = image2.to(device)\n",
    "    \n",
    "    # Create a wrapper function for the model that takes a single input\n",
    "    # This needs to be an nn.Module for Captum's hooks\n",
    "    class ModelWrapper(nn.Module):\n",
    "        def __init__(self, similarity_model_instance, fixed_image_tensor):\n",
    "            super().__init__()\n",
    "            self.similarity_model_instance = similarity_model_instance\n",
    "            self.fixed_image_tensor = fixed_image_tensor\n",
    "            self.similarity_model_instance.eval() # Ensure eval mode\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.similarity_model_instance(x, self.fixed_image_tensor)\n",
    "\n",
    "    wrapped_model_for_captum = ModelWrapper(model, image2).to(device)\n",
    "    wrapped_model_for_captum.eval()\n",
    "    \n",
    "    # Initialize the Occlusion attribution method\n",
    "    occlusion_attr = Occlusion( # Renamed to avoid conflict with captum.attr.Occlusion\n",
    "        wrapped_model_for_captum\n",
    "    )\n",
    "    \n",
    "    # Compute attributions\n",
    "    attributions = occlusion_attr.attribute(\n",
    "        image1,\n",
    "        strides=(3, stride, stride),  # (channels, height, width)\n",
    "        sliding_window_shapes=(3, patch_size, patch_size),\n",
    "        baselines=occlusion_value,\n",
    "        target=None,  # Corrected: Use None for scalar output per batch item\n",
    "    )\n",
    "    \n",
    "    # Convert attributions to sensitivity map\n",
    "    # The output of occlusion.attribute is typically [N, C, H, W]\n",
    "    # We want to see the impact, so taking the absolute difference or sum can be useful.\n",
    "    # Here, let's consider the sum of attributions across channels.\n",
    "    # A common way to interpret occlusion is that a large magnitude (positive or negative)\n",
    "    # in attribution for a region means occluding it changed the output significantly.\n",
    "    # The sign indicates direction. If baseline is 0, and output drops, attribution might be negative.\n",
    "    # Let's sum attributions and then take absolute for magnitude of change.\n",
    "    # Or, if we want to see \"what makes the score drop\", we might not take abs if original_score - perturbed_score is calculated.\n",
    "    # Captum's Occlusion gives attribution of occluded region towards output.\n",
    "    # A simple way to get a per-pixel map is to average over the channel dimension.\n",
    "    \n",
    "    sensitivity_map = attributions.squeeze(0).abs().mean(dim=0).cpu().detach() # .detach() is good practice\n",
    "    \n",
    "    # Normalize the sensitivity map for visualization\n",
    "    if sensitivity_map.max() > 0:\n",
    "        sensitivity_map = (sensitivity_map - sensitivity_map.min()) / (sensitivity_map.max() - sensitivity_map.min())\n",
    "    \n",
    "    return sensitivity_map.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a6c698",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_occlusion_sensitivity(image, sensitivity_map, title):\n",
    "    \"\"\"\n",
    "    Visualize the occlusion sensitivity map overlaid on the original image.\n",
    "    \n",
    "    Args:\n",
    "        image: Original PIL image\n",
    "        sensitivity_map: The computed sensitivity map\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Resize sensitivity map to match image dimensions\n",
    "    resized_map = cv2.resize(sensitivity_map, (image.size[0], image.size[1]))\n",
    "    \n",
    "    # Convert PIL image to numpy array and normalize\n",
    "    img_array = np.array(image) / 255.0\n",
    "    \n",
    "    # Create a heatmap visualization\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * resized_map), cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    \n",
    "    # Overlay the heatmap on the image\n",
    "    overlay = 0.7 * img_array + 0.3 * heatmap\n",
    "    overlay = overlay / np.max(overlay)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(resized_map, cmap='jet')\n",
    "    plt.title(\"Sensitivity Map\")\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(f\"Overlay - {title}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform occlusion sensitivity testing on same newt pair\n",
    "print(\"Performing occlusion sensitivity test for same newt...\")\n",
    "sensitivity_map_same = occlusion_sensitivity(\n",
    "    similarity_model, \n",
    "    newt1_tensor1, \n",
    "    newt1_tensor2, \n",
    "    patch_size=16, \n",
    "    stride=8\n",
    ")\n",
    "\n",
    "\n",
    "# Visualize occlusion sensitivity results\n",
    "visualize_occlusion_sensitivity(\n",
    "    newt1_img1,\n",
    "    sensitivity_map_same,\n",
    "    f\"Same Newt {random_newts[0]} - Regions Important for Similarity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MY occlusion sensitivity testing on same newt pair\n",
    "print(\"Performing MY occlusion sensitivity test for same newts...\")\n",
    "sensitivity_map_same = my_occlusion_sensitivity(\n",
    "    similarity_model,\n",
    "    newt1_tensor1,  \n",
    "    newt1_tensor2,  \n",
    "    patch_size=16,\n",
    "    stride=8\n",
    ")\n",
    "\n",
    "# Visualize occlusion sensitivity results\n",
    "visualize_occlusion_sensitivity(\n",
    "    newt1_img1,\n",
    "    sensitivity_map_same,\n",
    "    f\"Same Newt {random_newts[0]} - Regions Important for Similarity\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ece898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform occlusion sensitivity testing on different newt pair\n",
    "print(\"Performing occlusion sensitivity test for different newts...\")\n",
    "sensitivity_map_diff = occlusion_sensitivity(\n",
    "    similarity_model,\n",
    "    newt1_tensor1,  # Image of newt_ID_A (this one will be occluded)\n",
    "    newt2_tensor1,  # Image of newt_ID_B (this is the reference for similarity)\n",
    "    patch_size=16,\n",
    "    stride=8\n",
    ")\n",
    "\n",
    "# Visualize occlusion sensitivity results\n",
    "visualize_occlusion_sensitivity(\n",
    "    newt1_img1,\n",
    "    sensitivity_map_diff,\n",
    "    f\"Different Newts {random_newts[0]} vs {random_newts[1]} - Regions Important for Similarity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MY occlusion sensitivity testing on different newt pair\n",
    "print(\"Performing MY occlusion sensitivity test for different newts...\")\n",
    "sensitivity_map_diff = my_occlusion_sensitivity(\n",
    "    similarity_model,\n",
    "    newt1_tensor1,  # Image of newt_ID_A (this one will be occluded)\n",
    "    newt2_tensor1,  # Image of newt_ID_B (this is the reference for similarity)\n",
    "    patch_size=16,\n",
    "    stride=8\n",
    ")\n",
    "\n",
    "# Visualize occlusion sensitivity results\n",
    "visualize_occlusion_sensitivity(\n",
    "    newt1_img1,\n",
    "    sensitivity_map_diff,\n",
    "    f\"Different Newts {random_newts[0]} vs {random_newts[1]} - Regions Important for Similarity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7c49b",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5b86c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def integrated_gradients_similarity(model_instance, image1_tensor, image2_tensor, n_steps=50, target_output_idx=None):\n",
    "    \"\"\"\n",
    "    Compute Integrated Gradients for the first image with respect to the similarity\n",
    "    score with the second image.\n",
    "    \n",
    "    Args:\n",
    "        model_instance: The SimilarityModel instance (which is an nn.Module).\n",
    "        image1_tensor: Tensor of the first image (to attribute). Shape [1, C, H, W].\n",
    "        image2_tensor: Tensor of the second image (fixed reference). Shape [1, C, H, W].\n",
    "        n_steps: Number of steps for the integration.\n",
    "        target_output_idx: If model outputs multiple values, specify index. For scalar output, can be None.\n",
    "\n",
    "    Returns:\n",
    "        Attributions for image1_tensor.\n",
    "    \"\"\"\n",
    "    model_instance.eval() # Ensure the main model is in eval mode\n",
    "    image1_tensor = image1_tensor.to(device)\n",
    "    image2_tensor = image2_tensor.to(device)\n",
    "\n",
    "    # Ensure tensors require gradients\n",
    "    image1_tensor.requires_grad_()\n",
    "\n",
    "    # Define a wrapper nn.Module for Captum\n",
    "    class ModelWrapper(nn.Module):\n",
    "        def __init__(self, similarity_model_instance, fixed_image_tensor):\n",
    "            super().__init__()\n",
    "            self.similarity_model_instance = similarity_model_instance\n",
    "            self.fixed_image_tensor = fixed_image_tensor\n",
    "            # Ensure the passed model instance is also in eval mode if it wasn't already\n",
    "            self.similarity_model_instance.eval() \n",
    "        \n",
    "        def forward(self, img1_input):\n",
    "            return self.similarity_model_instance(img1_input, self.fixed_image_tensor)\n",
    "\n",
    "    wrapped_model = ModelWrapper(model_instance, image2_tensor).to(device)\n",
    "    wrapped_model.eval() \n",
    "\n",
    "    ig = IntegratedGradients(wrapped_model)\n",
    "    \n",
    "    baseline = torch.zeros_like(image1_tensor).to(device)\n",
    "    \n",
    "    attributions = ig.attribute(image1_tensor,\n",
    "                                baselines=baseline,\n",
    "                                target=target_output_idx, \n",
    "                                n_steps=n_steps,\n",
    "                                return_convergence_delta=False) \n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55581b6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_integrated_gradients(image_pil, attributions_tensor, title):\n",
    "    \"\"\"\n",
    "    Visualize Integrated Gradients attributions.\n",
    "    \"\"\"\n",
    "    # Convert attributions to numpy array and take the sum across color channels\n",
    "    attributions_np = attributions_tensor.squeeze().cpu().detach().numpy()\n",
    "    attributions_np = np.transpose(attributions_np, (1, 2, 0))\n",
    "    attribution_map = np.sum(np.abs(attributions_np), axis=2) # Sum absolute attributions across channels\n",
    "    \n",
    "    # Normalize the attribution map for visualization\n",
    "    if np.max(attribution_map) > 0:\n",
    "        attribution_map = (attribution_map - np.min(attribution_map)) / (np.max(attribution_map) - np.min(attribution_map))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    axes[0].imshow(image_pil)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    im = axes[1].imshow(attribution_map, cmap='inferno') # 'inferno' or 'viridis' are good choices\n",
    "    axes[1].set_title(\"Integrated Gradients Attribution\")\n",
    "    axes[1].axis('off')\n",
    "    fig.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726b0de",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Perform Integrated Gradients for the same newt pair\n",
    "# We are interested in how image1 contributes to similarity with image2\n",
    "print(\"Performing Integrated Gradients for same newt (image1 vs image2)...\")\n",
    "attributions_same_newt_img1 = integrated_gradients_similarity(\n",
    "    similarity_model,\n",
    "    newt1_tensor1, # Image to attribute\n",
    "    newt1_tensor2  # Fixed reference image\n",
    ")\n",
    "visualize_integrated_gradients(\n",
    "    newt1_img1, # PIL image corresponding to newt1_tensor1\n",
    "    attributions_same_newt_img1,\n",
    "    f\"IG: Newt {random_newts[0]} (Img 1) vs Newt {random_newts[0]} (Img 2)\"\n",
    ")\n",
    "\n",
    "# Perform Integrated Gradients for the different newt pair\n",
    "# We are interested in how image1 (from newt A) contributes to similarity with image1 (from newt B)\n",
    "print(\"\\nPerforming Integrated Gradients for different newts (newt A img1 vs newt B img1)...\")\n",
    "attributions_diff_newt_img1 = integrated_gradients_similarity(\n",
    "    similarity_model,\n",
    "    newt1_tensor1, # Image to attribute (from first newt)\n",
    "    newt2_tensor1  # Fixed reference image (from second newt)\n",
    ")\n",
    "visualize_integrated_gradients(\n",
    "    newt1_img1, # PIL image corresponding to newt1_tensor1\n",
    "    attributions_diff_newt_img1,\n",
    "    f\"IG: Newt {random_newts[0]} (Img 1) vs Newt {random_newts[1]} (Img 1)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51cd2b",
   "metadata": {},
   "source": [
    "## Blur Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a628f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def blur_perturbation_similarity(model_instance, image1_tensor, image2_tensor, patch_size=16, stride=8, blur_sigma=5):\n",
    "    \"\"\"\n",
    "    Perform perturbation-based saliency by blurring patches of image1 and observing\n",
    "    the change in similarity with image2.\n",
    "    \n",
    "    Args:\n",
    "        model_instance: The SimilarityModel instance.\n",
    "        image1_tensor: Tensor of the first image (to be perturbed). Shape [1, C, H, W].\n",
    "        image2_tensor: Tensor of the second image (fixed reference). Shape [1, C, H, W].\n",
    "        patch_size: Size of the patch to blur.\n",
    "        stride: Stride for moving the patch.\n",
    "        blur_sigma: Sigma for Gaussian blur.\n",
    "        \n",
    "    Returns:\n",
    "        Sensitivity map (higher values mean blurring that region decreased similarity more).\n",
    "    \"\"\"\n",
    "    model_instance.eval()\n",
    "    image1_tensor_cpu = image1_tensor.cpu() # Work with CPU tensor for easier numpy conversion and blurring\n",
    "    image2_tensor_dev = image2_tensor.to(device) # Keep image2 on device for model input\n",
    "\n",
    "    # Get the original similarity score\n",
    "    with torch.no_grad():\n",
    "        original_similarity = model_instance(image1_tensor.to(device), image2_tensor_dev).item()\n",
    "    \n",
    "    # Get image dimensions\n",
    "    _, c, h, w = image1_tensor_cpu.shape\n",
    "    \n",
    "    # Initialize sensitivity map\n",
    "    sensitivity_map = torch.zeros((h, w), device='cpu')\n",
    "    \n",
    "    # Create a blurred version of the entire image1 (used for replacing patches)\n",
    "    # Convert tensor to numpy for blurring: (C, H, W)\n",
    "    image1_numpy = image1_tensor_cpu.squeeze(0).numpy() \n",
    "    blurred_image1_numpy = np.zeros_like(image1_numpy)\n",
    "    for channel_idx in range(c):\n",
    "        blurred_image1_numpy[channel_idx, :, :] = gaussian_filter(image1_numpy[channel_idx, :, :], sigma=blur_sigma)\n",
    "    \n",
    "    # Compute number of patches\n",
    "    n_h_patches = (h - patch_size) // stride + 1\n",
    "    n_w_patches = (w - patch_size) // stride + 1\n",
    "    \n",
    "    total_patches = n_h_patches * n_w_patches\n",
    "    patch_count = 0\n",
    "    \n",
    "    print(f\"Starting blur perturbation: {total_patches} patches to process...\")\n",
    "    for i in range(0, h - patch_size + 1, stride):\n",
    "        for j in range(0, w - patch_size + 1, stride):\n",
    "            perturbed_image_numpy = image1_numpy.copy()\n",
    "            \n",
    "            # Replace the patch with the corresponding patch from the blurred image\n",
    "            perturbed_image_numpy[:, i:i+patch_size, j:j+patch_size] = \\\n",
    "                blurred_image1_numpy[:, i:i+patch_size, j:j+patch_size]\n",
    "            \n",
    "            # Convert back to tensor and move to device\n",
    "            perturbed_image_tensor = torch.from_numpy(perturbed_image_numpy).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                perturbed_similarity = model_instance(perturbed_image_tensor, image2_tensor_dev).item()\n",
    "            \n",
    "            sensitivity = original_similarity - perturbed_similarity\n",
    "            sensitivity_map[i:i+patch_size, j:j+patch_size] += sensitivity # Accumulate if patches overlap\n",
    "            \n",
    "            patch_count += 1\n",
    "            if patch_count % 20 == 0 or patch_count == total_patches:\n",
    "                print(f\"Processed {patch_count}/{total_patches} patches...\", end='\\r')\n",
    "    \n",
    "    print(f\"\\nCompleted blur perturbation.\")\n",
    "    \n",
    "    # Normalize the sensitivity map\n",
    "    if sensitivity_map.abs().max() > 0: # Check against absolute max to handle negative sensitivities too\n",
    "         # Center around 0 then scale, or just scale positive changes\n",
    "        if sensitivity_map.max() > sensitivity_map.min() and not (sensitivity_map.max() == 0 and sensitivity_map.min() == 0) :\n",
    "            sensitivity_map = (sensitivity_map - sensitivity_map.min()) / (sensitivity_map.max() - sensitivity_map.min())\n",
    "        elif sensitivity_map.max() > 0 : # if all values are same and positive\n",
    "             sensitivity_map = sensitivity_map / sensitivity_map.max()\n",
    "\n",
    "\n",
    "    return sensitivity_map.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21660904",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We can reuse visualize_occlusion_sensitivity, let's call it visualize_perturbation_map\n",
    "# or just use it as is if the title parameter is sufficient.\n",
    "# For consistency, I'll use the existing visualize_occlusion_sensitivity function.\n",
    "\n",
    "# Perform Blur Perturbation for the same newt pair\n",
    "print(\"Performing Blur Perturbation for same newt (image1 vs image2)...\")\n",
    "blur_map_same_newt = blur_perturbation_similarity(\n",
    "    similarity_model,\n",
    "    newt1_tensor1, \n",
    "    newt1_tensor2,\n",
    "    patch_size=24, # Larger patch might be more informative for blur\n",
    "    stride=12,\n",
    "    blur_sigma=5\n",
    ")\n",
    "visualize_occlusion_sensitivity( # Reusing the visualization function\n",
    "    newt1_img1,\n",
    "    blur_map_same_newt,\n",
    "    f\"Blur Perturbation: Newt {random_newts[0]} (Img 1) vs Newt {random_newts[0]} (Img 2)\"\n",
    ")\n",
    "\n",
    "# Perform Blur Perturbation for the different newt pair\n",
    "print(\"\\nPerforming Blur Perturbation for different newts (newt A img1 vs newt B img1)...\")\n",
    "blur_map_diff_newt = blur_perturbation_similarity(\n",
    "    similarity_model,\n",
    "    newt1_tensor1, \n",
    "    newt2_tensor1,\n",
    "    patch_size=24,\n",
    "    stride=12,\n",
    "    blur_sigma=5\n",
    ")\n",
    "visualize_occlusion_sensitivity( # Reusing the visualization function\n",
    "    newt1_img1,\n",
    "    blur_map_diff_newt,\n",
    "    f\"Blur Perturbation: Newt {random_newts[0]} (Img 1) vs Newt {random_newts[1]} (Img 1)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

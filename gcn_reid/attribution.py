"""Notebook for attribution testing"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_attribution.ipynb.

# %% auto 0
__all__ = ['my_occlusion_sensitivity']

# %% ../nbs/00_attribution.ipynb 15
def my_occlusion_sensitivity(model, image1, image2, patch_size=16, stride=8, occlusion_value=0, device=None):
    """
    Perform occlusion sensitivity test on the first image to see which regions
    affect similarity with the second image.
    
    Args:
        model: The similarity model
        image1: First image tensor (to be occluded) - shape [1, C, H, W]
        image2: Second image tensor - shape [1, C, H, W]
        patch_size: Size of the occlusion patch
        stride: Stride for moving the occlusion patch
        occlusion_value: Value used for occlusion (default: 0)
        
    Returns:
        Sensitivity map showing which regions, when occluded, affect similarity the most
    """

    import torch

    # Move tensors to the right device
    if device is not None:
        image1 = image1.to(device)
        image2 = image2.to(device)
    
    # Get the original similarity score
    with torch.no_grad():
        original_similarity = model(image1, image2).item()
    
    # Get image dimensions
    _, c, h, w = image1.shape
    
    # Initialize sensitivity map
    sensitivity_map = torch.zeros((h, w), device='cpu')
    
    # Compute number of patches
    n_h_patches = (h - patch_size) // stride + 1
    n_w_patches = (w - patch_size) // stride + 1
    
    # Create progress counter
    total_patches = n_h_patches * n_w_patches
    patch_count = 0
    
    # Slide the occlusion window over the image
    for i in range(0, h - patch_size + 1, stride):
        for j in range(0, w - patch_size + 1, stride):
            # Create a copy of the image
            occluded_image = image1.clone()
            
            # Apply occlusion
            occluded_image[0, :, i:i+patch_size, j:j+patch_size] = occlusion_value
            
            # Compute the similarity with occlusion
            with torch.no_grad():
                occluded_similarity = model(occluded_image, image2).item()
            
            # Calculate the difference (sensitivity)
            sensitivity = original_similarity - occluded_similarity
            
            # Update the sensitivity map
            sensitivity_map[i:i+patch_size, j:j+patch_size] += sensitivity
            
            # Update progress counter
            patch_count += 1
            if patch_count % 10 == 0:
                print(f"Processed {patch_count}/{total_patches} patches", end='\r')
    
    print(f"\nCompleted occlusion testing - {total_patches} patches processed.")
    
    # Normalize the sensitivity map for visualization
    if sensitivity_map.max() > 0:
        sensitivity_map = (sensitivity_map - sensitivity_map.min()) / (sensitivity_map.max() - sensitivity_map.min())
    
    return sensitivity_map.numpy()
